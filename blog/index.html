<!DOCTYPE html>
<html lang="en-us">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>ğŸ‘¨â€ğŸ’» Blog | Wyatt Walsh</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="ğŸ‘¨â€ğŸ’» Blog" />
<meta name="author" content="Wyatt Walsh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media." />
<meta property="og:description" content="Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media." />
<link rel="canonical" href="https://wwalsh.io/blog/" />
<meta property="og:url" content="https://wwalsh.io/blog/" />
<meta property="og:site_name" content="Wyatt Walsh" />
<link rel="next" href="https://wwalsh.io/post/2/" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ğŸ‘¨â€ğŸ’» Blog" />
<meta name="twitter:site" content="@wyattowalsh" />
<meta name="twitter:creator" content="@wyattowalsh" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Wyatt Walsh"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://wwalsh.io/public/favicon/favicon-32x32.png"},"name":"Wyatt Walsh"},"description":"Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media.","@type":"WebPage","url":"https://wwalsh.io/blog/","headline":"ğŸ‘¨â€ğŸ’» Blog","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FJCYWXT2WF"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FJCYWXT2WF');
  </script>


  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      ğŸ‘¨â€ğŸ’» Blog &middot; Wyatt Walsh
    
  </title>

  

  <link rel="stylesheet" href="https://wwalsh.io/public/css/poole.css">
  <link rel="stylesheet" href="https://wwalsh.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://wwalsh.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> -->
  <script src="https://kit.fontawesome.com/ebaeac9722.js" crossorigin="anonymous"></script>
  <link rel="apple-touch-icon-precomposed.png" sizes="180x180" href="public/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="public/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="public/favicon/favicon-16x16.png">
  <!-- <link rel="manifest" href="/site.webmanifest"> -->
  <link rel="shortcut icon" href="public/favicon/favicon.ico">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://wwalsh.io/atom.xml">


<!-- Icons -->
<!--   <link rel="apple-touch-icon-precomposed" sizes="144x144" href="public/apple-touch-icon-precomposed.png"> -->
  <!-- <link rel="shortcut icon" href="/public/favicon.ico"> -->



  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'G-FJCYWXT2WF', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="theme-base-0d">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->

<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">
<!-- Toggleable sidebar -->
<body class="sidebar-overlay">
<div class="sidebar" id="sidebar">

 <!--  <br><br> 

<div id="contact-list" style="text-align:center">
    
      <a target="_blank" rel="noopener noreferrer" href="https://github.com/wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-github-alt fa-stack-1x"></i>
        </span>
      </a>
    

    
      <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-twitter fa-stack-1x"></i>
        </span>
      </a>
    

    
      <a target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-linkedin fa-stack-1x"></i>
        </span>
      </a>
    
    
      <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh@gmail.com">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-envelope fa-stack-1x"></i>
        </span>
      </a>
    
    
      <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-stack-overflow fa-stack-1x"></i>
        </span>
  
  
      <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-kaggle fa-stack-1x"></i>
        </span>
  
  
      <a target="_blank" rel="noopener noreferrer" href="mailto:@wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-medium fa-stack-1x"></i>
        </span>
  
</div>
 <img align="middle" src="/public/favicon/android-chrome-512x512.png" style="text-align: center; padding-top:0;"/>
  
 -->
<div class="sidebar-item" id="contact-list" style="text-align:center; font-size: 275%; padding-bottom: 0">
  <!-- <p> Wyatt Walsh </p> -->
  <p align='center' style="margin:0; ">Wyatt Walsh</p>
</div>
<!-- <hr style="width:85%; line-height:30px; margin: auto;"> -->
<br>

<!-- 
  <div class="sidebar-item">
    <p>Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media.</p>
  </div> -->
    <!-- 
  <div class="sidebar-item">
    
  </div>
 -->
  <nav class="sidebar-nav"> 
    <p style="font-size:110%; text-align:center; padding-bottom:0; margin-bottom:0;"><u><b> Sections </b></u></p>
    <hr style="width:100%; margin: auto; border: 0.25px solid;">
    <a class="sidebar-nav-item" href="https://wwalsh.io/">&#127968; Home</a>

    

    
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
          
          
          <a class="sidebar-nav-item" href="https://wwalsh.io/about/">ğŸ¤” About</a>
        
        
      
    
      
    
      
        
          
            
      
        
          
          
            
      
        
          
          
          <a class="sidebar-nav-item active" href="https://wwalsh.io/blog/index.html">ğŸ‘¨â€ğŸ’» Blog</a>
        
        
      
    
      
        
        
          <a target="_blank" rel="noopener noreferrer" class="sidebar-nav-item" href="https://makeuseofdata.com"> ğŸ“” Notes on Data Science </a>
        
      
    
      
    
      
    
      
        
        
      
    
      
    
    
    <!-- <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> -->
  </nav>


  <div style="text-align:center; font-size: 225%;">
    <hr style="width:100%; margin: auto; border: 0.25px solid;">
  <!--   <hr style="width:85%; margin: auto; padding-top:0;">
 -->    
<div id="social-media">
    
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://www.twitter.com/wyattowalsh" title="Twitter"><i class="fa fa-twitter"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/wyattowalsh" title="LinkedIn"><i class="fa fa-linkedin"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://github.com/wyattowalsh" title="GitHub"><i class="fa fa-github"></i></a>
        
        
        <br>
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/story/wyattowalsh" title="Stackoverflow"><i class="fa fa-stack-overflow"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://kaggle.com/wyattowalsh" title="Kaggle"><i class="fab fa-kaggle"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@wyattowalsh" title="Medium"><i class="fa fa-medium"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://open.spotify.com/user/122096382" title="Spotify"><i class="fa fa-spotify"></i></a>
        
        
    
</div>

    <p style="font-size: 42%; padding-bottom:3pt; margin-bottom:3pt;"> ~ Click Above to Connect With Me ~ </p>
    <hr style="width:100%; margin: auto; border: 0.25px solid;">
  </div>


  <div class="sidebar-item">
    <p style="position: fixed; bottom: 0;">
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>
</body>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Wyatt Walsh</a>
            <small>Reach out, let's build some cool stuff!</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="page" style="width: 100%; overflow-x: auto;">
<!--   <h1 class="page-title">ğŸ‘¨â€ğŸ’» Blog</h1> -->
  <h2> Latest Blog Posts: </h2>

<!-- <source url="https://medium.com/feed/@wyattowalsh">Latest Blog Posts</source> -->



<div style="background-color:#e1ebf0;">
	<div class="row" style="margin-left: 25px;margin-right: 25px;">
		
		<h3>2020-01-15 | Published in <a target="_blank" rel="noopener noreferrer" href="https://towardsdatascience.com/"><b><i>Towards Data Science</i></b> </a></h3>
		<hr>
		<h3> Regularized Linear Regression Models</h3>
		<p><h3>Implementing Pathwise Coordinate Descent For The Lasso and The Elastic Net In Python UsingÂ NumPy</h3><h4>Explanations for Solving Some of the Most Popular Supervised Learning Algorithms</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cju3fHmuTtt5w4AFV7lfFg.png" /><figcaption>Model Coefficient Value Changes With Growing Regularization Penalty Values (<em>Image byÂ author)</em></figcaption></figure><p>Hey there!Â ğŸ‘‹</p><p>Welcome to the final part of a three-part deep-dive on <strong>regularized linear regression modeling</strong>! In<strong> </strong><a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c"><strong>part one</strong></a>, linear modeling was established with the derivation of OLS showing how to solve for model coefficients to make predictions of the response given new feature data. Next, in <a href="https://towardsdatascience.com/regularized-linear-regression-models-44572e79a1b5"><strong>part two</strong></a>, <strong><em>overfitting </em></strong>issues of the OLS model were discussed and<strong><em> Ridge Regression </em></strong>was presented as a technique to help reduce overfitting through regularization. Building off the same concept as Ridge Regression,<strong><em> the Lasso</em></strong> and <strong><em>the Elastic Net </em></strong>are now presented. The series concludes with general considerations of use cases for the techniques presented.</p><p>The models and included implementations were tested on a wine quality prediction dataset of which the code and results can be viewed at the project repository <a href="http://github.com/wyattowalsh/regularized-linear-regression-deep-dive"><strong>here</strong></a><strong>.</strong></p><h3>The Lasso for Regression</h3><p>The Lasso, or <strong><em>Least Absolute Shrinkage and Selection Operator</em></strong>, includes the addition of an <strong><em>Lâ‚</em></strong> penalty to the OLS loss function (<a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c">Part One</a>: Eq. #7), bringing selective model parameters to zero for a large enough value of an associated tuning parameter, <strong><em>Î»</em></strong>.<strong><em> </em></strong>In other words, <strong><em>the Lasso</em></strong> <strong>performs automated feature selection </strong>producing a vector of model coefficients with sparsity (amount of elements that are zero) varying on the magnitude of a tuning parameter.</p><p><strong><em>The Lasso</em></strong> loss function can be formalized as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/616/1*n6qJO9L9ZOaPfkbtzNcsMQ.png" /><figcaption>Equation #1</figcaption></figure><p>Similar to previous cases, an intercept term can be included through data augmentation of the design matrix with a column of <strong>1</strong>s. Furthermore, formulating the problem as a <em>least-squares</em> optimization problem produces:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/528/1*B-Z2-_00fCCDq-dQNBlRpA.png" /><figcaption>Equation #2</figcaption></figure><p>However, unlike previous cases, no closed-form solution exists for this problem. This is due to the fact that the addition of the <strong><em>Lâ‚</em></strong> penalty makes the function no longer continuously differentiable because of the non-smooth absolute component. To remedy this issue, a discrete optimization technique needs to be applied to search for an optimal solution.</p><p>Numerous algorithms exist to this end, such as<strong><em> LARS</em></strong> (Least Angle Regression) and <strong><em>Forward Stepwise Regression</em></strong>, however, the <strong><em>Pathwise Coordinate Descent</em></strong> algorithm is leveraged within this work. In short, this algorithm optimizes a parameter at a time holding all other parameters constant.</p><h4>Pathwise Coordinate Descent</h4><p>Before beginning the algorithm, all features should be standardized to have zero mean and variance of one. From there, a <strong><em>p+1</em></strong> length coefficient vector is initialized to zero. Cycles are then run across all coefficients until convergenceâ€Šâ€”â€Šwhere values of the coefficients stabilize and do not change more than a certain toleranceâ€Šâ€”â€Šis reached. Within each cycle, for every coefficient, an update is calculated and subsequently has the soft-thresholding operator applied toÂ it.</p><p>The simplest form of <strong><em>Coordinate Descent </em>updates </strong>calculatesâ€Šâ€”â€Šfor each coefficientâ€Šâ€”â€Šthe simple (single variable as opposed to multiple regression) least-squares coefficient value using the partial residuals across all other features in the design matrix. Partial residuals, in this case, are foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/216/1*EFiIYLkOAKwFeoEv38cj4Q.png" /><figcaption>Equation #3</figcaption></figure><p>Therefore, the estimate for a particular coefficient value can be foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/188/1*Tf7Lg7RyvsAy-Ez6R_rS5Q.png" /><figcaption>Equation #4</figcaption></figure><p>Now, the penalty, as dictated by the tuning parameter, is included in the model through the soft-thresholding operator. This is expressed as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*N02FKWLcTEdyS9tMg2sn1w.png" /><figcaption>Equation #5</figcaption></figure><p><strong><em>Naive updates</em></strong> can be utilized for improved efficiency. These updates are foundÂ via:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/228/1*-9CpaMzXYs465_hSYhz64A.png" /><figcaption>Equation #6</figcaption></figure><p>where <strong><em>ráµ¢ </em></strong>is the current model residual for all samples,Â <strong><em>n</em></strong>.</p><p>When the number of samples is much greater than the number of features (<strong><em>n</em></strong> &gt;&gt; <strong><em>p</em></strong>), further efficiency improvements can be derived by using <strong>covariance updates</strong>. For these updates, the first term of the <strong><em>Naive update</em></strong> equation above (Eq. #6) is replaced as shownÂ by:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/396/1*fjqGZtZM4up6crG8R-f-Og.png" /><figcaption>Equation #7</figcaption></figure><p>Utilizing <strong><em>warm starts</em></strong><em> </em>can bring efficiency boosts as well. Using <strong><em>warm starts</em></strong>, a sequence of models are fittedâ€Šâ€”â€Šwith tuning parameter values from a max tuning parameter value down to a minimum tuning parameter value that is some small factor (thousandth) of the max valueâ€Šâ€”â€Šinitializing the coefficients of each iteration to the solution of the last iteration. In many cases, it is actually faster to fit this path of models than a single model for some small <strong><em>Î». </em></strong>Thus the path can be expressed as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/244/1*L8Q-Pg_7oaGJ1i_V14bpcQ.png" /><figcaption>Equation #8</figcaption></figure><p>where <strong><em>ğœ– </em></strong>is typically 0.001 and there are 100 values spaced on a logÂ scale.</p><p>Furthermore, the max value of the tuning parameter to begin the path at can be found by finding the minimum value that will bring the estimates for all model coefficients to zero. This is since any values above this value will result in total sparsity of the coefficient vector. The max value of the path (starting point) can be foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/220/1*aFx9cbsJ23otuk1SYNLo6A.png" /><figcaption>Equation #9</figcaption></figure><p>By providing a starting place to begin searching for optimality, warm starting can many times speed up convergence and also puts the <em>pathwise</em> in the <strong><em>Pathwise</em> <em>Coordinate Descent</em></strong> algorithm.</p><h4>Implementation of the Lasso In Python UsingÂ NumPy</h4><p>One possible way to implement <strong><em>pathwise coordinate descen</em></strong>t for <strong><em>the Lasso </em></strong>(with options for tuning the convergence tolerance, path length, and returning the path)Â is:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6a17e79eaf5822803154eef272588c34/href">https://medium.com/media/6a17e79eaf5822803154eef272588c34/href</a></iframe><h3>The ElasticÂ Net</h3><p>In this form of regularized linear regression, the OLS loss function is changed by the addition of both an <strong><em>Lâ‚ </em></strong>and <strong><em>Lâ‚‚</em></strong> penalty to the loss function with tuning parameters controlling the intensity of regularization and the balance between the different penalties. This loss function can be formalized as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/868/1*2XfDiqMRQW4A4FPanjtDbQ.png" /><figcaption>Equation #10</figcaption></figure><p>Having both <strong><em>Lâ‚ </em></strong>and <strong><em>Lâ‚‚</em></strong> penalties, <strong><em>the Elastic Net</em></strong> serves to deliver a compromise between Ridge regression and the Lasso, bringing coefficients towards zero and selectively to zero. Here, <strong><em>Î±</em></strong> can be considered as the parameter determining the ratio of <strong><em>Lâ‚</em></strong> penalty to add, whereas <strong><em>Î»</em></strong> can be thought of as the intensity of regularization toÂ apply.</p><p><strong><em>The Elastic Net</em></strong> loss function is also used to formalize a <em>least-squares</em> optimization problem:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*pgNCXwfeBiRTg20dp77qgw.png" /><figcaption>Equation #11</figcaption></figure><p>Similarly to the Lasso, an intercept is included through design matrix augmentation, a 1/(2n) term is added for mathematical completeness, and <strong><em>pathwise coordinate descent</em></strong> is implemented to solve since a closed-form solution does not exist due to the <strong><em>Lâ‚</em></strong> penaltyÂ term.</p><p>Just as in <strong><em>the Lasso, pathwise coordinate descent</em></strong> is used to solve for model coefficient estimates however changes need to be made to the update equations to account for the dual penalties. In this case, the <strong><em>j</em></strong>th coefficient value obtained after soft-thresholding is now foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/192/1*n5aQwN0qNsJo3jR_Fztbvg.png" /><figcaption>Equation #12</figcaption></figure><p>The soft-thresholding operator is the same operator applied in <strong><em>the Lasso</em></strong> update (Eq.Â #5):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/228/1*17U2oOQGydXu8A2FbiU4jg.png" /><figcaption>Equation #13</figcaption></figure><p>Furthermore, <strong>naive updates </strong>or<em> </em><strong>covariance updates </strong>can be used<em> </em>along with <strong>warm starts</strong>. For <strong>warm starts</strong>, the max <strong><em>Î» </em></strong>value can be calculated as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/220/1*OWsgat02uTwhal5os82CWg.png" /><figcaption>Equation #14</figcaption></figure><h4>Implementation in Python UsingÂ NumPy</h4><p>One possible way to implement <strong><em>pathwise coordinate descent</em></strong> to solve <strong><em>the Elastic Net</em></strong> (with options for tuning the convergence tolerance, path length, and returning the path)Â is:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b3c029a3f816154237ae00881fae28a1/href">https://medium.com/media/b3c029a3f816154237ae00881fae28a1/href</a></iframe><h3>Conclusion</h3><p>Throughout this series, different regularized forms of linear regression have been examined as tools to overcome the tendency to overfit training data of the <strong><em>Ordinary Least Squares </em></strong>model. <strong><em>The Elastic Netâ€Š</em></strong>â€”â€Šdue to its balance between regularization varietiesâ€Šâ€”â€Štends to be the most robust to aid the <strong>overfitting</strong> issue, however, <strong><em>the Lasso</em></strong> certainly can prove helpful in many situations due to its <strong>automated feature selection</strong>. <strong><em>Ridge Regression</em></strong> is also a good tool to use to ensure a reduction in possible model overfitting as it shrinks model coefficients towards zero, reducing model variance.</p><p>The header image of this series well demonstrates the difference between <strong><em>Ridge Regression</em></strong> and <strong><em>the Lasso</em></strong>, as it can be seen that the model coefficients all shrink towards zero on the left for the <strong><em>Ridge Regression</em></strong> case, and, on the right, coefficients are being brought to zero in a selective order for <strong><em>the Lasso</em></strong>Â case.</p><p>Congratulations! ğŸ‰ğŸŠğŸ¥³</p><p>You made it to the end of theÂ series!</p><p>I hope that these posts were informative and helpful for you to learn more about <strong><em>regularized linear regression</em></strong> and the necessary <strong>optimization</strong> needed to solve the associated models.</p><p>See <a href="https://github.com/wyattowalsh/regularized-linear-regression-deep-dive/blob/master/SOURCES.md"><strong>here</strong></a> for the different sources utilized to create this series ofÂ posts.</p><p>If you are just now beginning the series, make sure to check out <a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c"><strong>part one</strong></a> and <a href="https://towardsdatascience.com/regularized-linear-regression-models-44572e79a1b5"><strong>partÂ two</strong></a>!</p><p>Please leave a comment if you would like! I am always trying to improve my posts (logically, syntactically, or otherwise) and am happy to discuss anything related!Â ğŸ‘‹</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dcf5aa662ab9" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/regularized-linear-regression-models-dcf5aa662ab9">Regularized Linear Regression Models</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
		<br>
	</div>
</div>




<!-- This loops through the paginated posts -->
<!-- 

  <h1><h3>Implementing Pathwise Coordinate Descent For The Lasso and The Elastic Net In Python UsingÂ NumPy</h3><h4>Explanations for Solving Some of the Most Popular Supervised Learning Algorithms</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cju3fHmuTtt5w4AFV7lfFg.png" /><figcaption>Model Coefficient Value Changes With Growing Regularization Penalty Values (<em>Image byÂ author)</em></figcaption></figure><p>Hey there!Â ğŸ‘‹</p><p>Welcome to the final part of a three-part deep-dive on <strong>regularized linear regression modeling</strong>! In<strong> </strong><a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c"><strong>part one</strong></a>, linear modeling was established with the derivation of OLS showing how to solve for model coefficients to make predictions of the response given new feature data. Next, in <a href="https://towardsdatascience.com/regularized-linear-regression-models-44572e79a1b5"><strong>part two</strong></a>, <strong><em>overfitting </em></strong>issues of the OLS model were discussed and<strong><em> Ridge Regression </em></strong>was presented as a technique to help reduce overfitting through regularization. Building off the same concept as Ridge Regression,<strong><em> the Lasso</em></strong> and <strong><em>the Elastic Net </em></strong>are now presented. The series concludes with general considerations of use cases for the techniques presented.</p><p>The models and included implementations were tested on a wine quality prediction dataset of which the code and results can be viewed at the project repository <a href="http://github.com/wyattowalsh/regularized-linear-regression-deep-dive"><strong>here</strong></a><strong>.</strong></p><h3>The Lasso for Regression</h3><p>The Lasso, or <strong><em>Least Absolute Shrinkage and Selection Operator</em></strong>, includes the addition of an <strong><em>Lâ‚</em></strong> penalty to the OLS loss function (<a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c">Part One</a>: Eq. #7), bringing selective model parameters to zero for a large enough value of an associated tuning parameter, <strong><em>Î»</em></strong>.<strong><em> </em></strong>In other words, <strong><em>the Lasso</em></strong> <strong>performs automated feature selection </strong>producing a vector of model coefficients with sparsity (amount of elements that are zero) varying on the magnitude of a tuning parameter.</p><p><strong><em>The Lasso</em></strong> loss function can be formalized as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/616/1*n6qJO9L9ZOaPfkbtzNcsMQ.png" /><figcaption>Equation #1</figcaption></figure><p>Similar to previous cases, an intercept term can be included through data augmentation of the design matrix with a column of <strong>1</strong>s. Furthermore, formulating the problem as a <em>least-squares</em> optimization problem produces:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/528/1*B-Z2-_00fCCDq-dQNBlRpA.png" /><figcaption>Equation #2</figcaption></figure><p>However, unlike previous cases, no closed-form solution exists for this problem. This is due to the fact that the addition of the <strong><em>Lâ‚</em></strong> penalty makes the function no longer continuously differentiable because of the non-smooth absolute component. To remedy this issue, a discrete optimization technique needs to be applied to search for an optimal solution.</p><p>Numerous algorithms exist to this end, such as<strong><em> LARS</em></strong> (Least Angle Regression) and <strong><em>Forward Stepwise Regression</em></strong>, however, the <strong><em>Pathwise Coordinate Descent</em></strong> algorithm is leveraged within this work. In short, this algorithm optimizes a parameter at a time holding all other parameters constant.</p><h4>Pathwise Coordinate Descent</h4><p>Before beginning the algorithm, all features should be standardized to have zero mean and variance of one. From there, a <strong><em>p+1</em></strong> length coefficient vector is initialized to zero. Cycles are then run across all coefficients until convergenceâ€Šâ€”â€Šwhere values of the coefficients stabilize and do not change more than a certain toleranceâ€Šâ€”â€Šis reached. Within each cycle, for every coefficient, an update is calculated and subsequently has the soft-thresholding operator applied toÂ it.</p><p>The simplest form of <strong><em>Coordinate Descent </em>updates </strong>calculatesâ€Šâ€”â€Šfor each coefficientâ€Šâ€”â€Šthe simple (single variable as opposed to multiple regression) least-squares coefficient value using the partial residuals across all other features in the design matrix. Partial residuals, in this case, are foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/216/1*EFiIYLkOAKwFeoEv38cj4Q.png" /><figcaption>Equation #3</figcaption></figure><p>Therefore, the estimate for a particular coefficient value can be foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/188/1*Tf7Lg7RyvsAy-Ez6R_rS5Q.png" /><figcaption>Equation #4</figcaption></figure><p>Now, the penalty, as dictated by the tuning parameter, is included in the model through the soft-thresholding operator. This is expressed as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*N02FKWLcTEdyS9tMg2sn1w.png" /><figcaption>Equation #5</figcaption></figure><p><strong><em>Naive updates</em></strong> can be utilized for improved efficiency. These updates are foundÂ via:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/228/1*-9CpaMzXYs465_hSYhz64A.png" /><figcaption>Equation #6</figcaption></figure><p>where <strong><em>ráµ¢ </em></strong>is the current model residual for all samples,Â <strong><em>n</em></strong>.</p><p>When the number of samples is much greater than the number of features (<strong><em>n</em></strong> &gt;&gt; <strong><em>p</em></strong>), further efficiency improvements can be derived by using <strong>covariance updates</strong>. For these updates, the first term of the <strong><em>Naive update</em></strong> equation above (Eq. #6) is replaced as shownÂ by:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/396/1*fjqGZtZM4up6crG8R-f-Og.png" /><figcaption>Equation #7</figcaption></figure><p>Utilizing <strong><em>warm starts</em></strong><em> </em>can bring efficiency boosts as well. Using <strong><em>warm starts</em></strong>, a sequence of models are fittedâ€Šâ€”â€Šwith tuning parameter values from a max tuning parameter value down to a minimum tuning parameter value that is some small factor (thousandth) of the max valueâ€Šâ€”â€Šinitializing the coefficients of each iteration to the solution of the last iteration. In many cases, it is actually faster to fit this path of models than a single model for some small <strong><em>Î». </em></strong>Thus the path can be expressed as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/244/1*L8Q-Pg_7oaGJ1i_V14bpcQ.png" /><figcaption>Equation #8</figcaption></figure><p>where <strong><em>ğœ– </em></strong>is typically 0.001 and there are 100 values spaced on a logÂ scale.</p><p>Furthermore, the max value of the tuning parameter to begin the path at can be found by finding the minimum value that will bring the estimates for all model coefficients to zero. This is since any values above this value will result in total sparsity of the coefficient vector. The max value of the path (starting point) can be foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/220/1*aFx9cbsJ23otuk1SYNLo6A.png" /><figcaption>Equation #9</figcaption></figure><p>By providing a starting place to begin searching for optimality, warm starting can many times speed up convergence and also puts the <em>pathwise</em> in the <strong><em>Pathwise</em> <em>Coordinate Descent</em></strong> algorithm.</p><h4>Implementation of the Lasso In Python UsingÂ NumPy</h4><p>One possible way to implement <strong><em>pathwise coordinate descen</em></strong>t for <strong><em>the Lasso </em></strong>(with options for tuning the convergence tolerance, path length, and returning the path)Â is:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6a17e79eaf5822803154eef272588c34/href">https://medium.com/media/6a17e79eaf5822803154eef272588c34/href</a></iframe><h3>The ElasticÂ Net</h3><p>In this form of regularized linear regression, the OLS loss function is changed by the addition of both an <strong><em>Lâ‚ </em></strong>and <strong><em>Lâ‚‚</em></strong> penalty to the loss function with tuning parameters controlling the intensity of regularization and the balance between the different penalties. This loss function can be formalized as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/868/1*2XfDiqMRQW4A4FPanjtDbQ.png" /><figcaption>Equation #10</figcaption></figure><p>Having both <strong><em>Lâ‚ </em></strong>and <strong><em>Lâ‚‚</em></strong> penalties, <strong><em>the Elastic Net</em></strong> serves to deliver a compromise between Ridge regression and the Lasso, bringing coefficients towards zero and selectively to zero. Here, <strong><em>Î±</em></strong> can be considered as the parameter determining the ratio of <strong><em>Lâ‚</em></strong> penalty to add, whereas <strong><em>Î»</em></strong> can be thought of as the intensity of regularization toÂ apply.</p><p><strong><em>The Elastic Net</em></strong> loss function is also used to formalize a <em>least-squares</em> optimization problem:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/1*pgNCXwfeBiRTg20dp77qgw.png" /><figcaption>Equation #11</figcaption></figure><p>Similarly to the Lasso, an intercept is included through design matrix augmentation, a 1/(2n) term is added for mathematical completeness, and <strong><em>pathwise coordinate descent</em></strong> is implemented to solve since a closed-form solution does not exist due to the <strong><em>Lâ‚</em></strong> penaltyÂ term.</p><p>Just as in <strong><em>the Lasso, pathwise coordinate descent</em></strong> is used to solve for model coefficient estimates however changes need to be made to the update equations to account for the dual penalties. In this case, the <strong><em>j</em></strong>th coefficient value obtained after soft-thresholding is now foundÂ as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/192/1*n5aQwN0qNsJo3jR_Fztbvg.png" /><figcaption>Equation #12</figcaption></figure><p>The soft-thresholding operator is the same operator applied in <strong><em>the Lasso</em></strong> update (Eq.Â #5):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/228/1*17U2oOQGydXu8A2FbiU4jg.png" /><figcaption>Equation #13</figcaption></figure><p>Furthermore, <strong>naive updates </strong>or<em> </em><strong>covariance updates </strong>can be used<em> </em>along with <strong>warm starts</strong>. For <strong>warm starts</strong>, the max <strong><em>Î» </em></strong>value can be calculated as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/220/1*OWsgat02uTwhal5os82CWg.png" /><figcaption>Equation #14</figcaption></figure><h4>Implementation in Python UsingÂ NumPy</h4><p>One possible way to implement <strong><em>pathwise coordinate descent</em></strong> to solve <strong><em>the Elastic Net</em></strong> (with options for tuning the convergence tolerance, path length, and returning the path)Â is:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b3c029a3f816154237ae00881fae28a1/href">https://medium.com/media/b3c029a3f816154237ae00881fae28a1/href</a></iframe><h3>Conclusion</h3><p>Throughout this series, different regularized forms of linear regression have been examined as tools to overcome the tendency to overfit training data of the <strong><em>Ordinary Least Squares </em></strong>model. <strong><em>The Elastic Netâ€Š</em></strong>â€”â€Šdue to its balance between regularization varietiesâ€Šâ€”â€Štends to be the most robust to aid the <strong>overfitting</strong> issue, however, <strong><em>the Lasso</em></strong> certainly can prove helpful in many situations due to its <strong>automated feature selection</strong>. <strong><em>Ridge Regression</em></strong> is also a good tool to use to ensure a reduction in possible model overfitting as it shrinks model coefficients towards zero, reducing model variance.</p><p>The header image of this series well demonstrates the difference between <strong><em>Ridge Regression</em></strong> and <strong><em>the Lasso</em></strong>, as it can be seen that the model coefficients all shrink towards zero on the left for the <strong><em>Ridge Regression</em></strong> case, and, on the right, coefficients are being brought to zero in a selective order for <strong><em>the Lasso</em></strong>Â case.</p><p>Congratulations! ğŸ‰ğŸŠğŸ¥³</p><p>You made it to the end of theÂ series!</p><p>I hope that these posts were informative and helpful for you to learn more about <strong><em>regularized linear regression</em></strong> and the necessary <strong>optimization</strong> needed to solve the associated models.</p><p>See <a href="https://github.com/wyattowalsh/regularized-linear-regression-deep-dive/blob/master/SOURCES.md"><strong>here</strong></a> for the different sources utilized to create this series ofÂ posts.</p><p>If you are just now beginning the series, make sure to check out <a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c"><strong>part one</strong></a> and <a href="https://towardsdatascience.com/regularized-linear-regression-models-44572e79a1b5"><strong>partÂ two</strong></a>!</p><p>Please leave a comment if you would like! I am always trying to improve my posts (logically, syntactically, or otherwise) and am happy to discuss anything related!Â ğŸ‘‹</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dcf5aa662ab9" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/regularized-linear-regression-models-dcf5aa662ab9">Regularized Linear Regression Models</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></h1>
 -->
<!-- 
<ul>
  
  
  <li>
    <a href="/blog/../post/2/">Older</a>
  </li>
  
</ul>
 -->
<div align="center" style="background-color:#d0e0e8;">

<ul>
  
  
    
        <a href="/blog/index.html" title="ğŸ‘¨â€ğŸ’» Blog"> 1 </a>
  
    
        <a href="/blog/../post/2/index.html" title="ğŸ‘¨â€ğŸ’» Blog - page 2"> 2 </a>
  
    
        <a href="/blog/../post/3/index.html" title="ğŸ‘¨â€ğŸ’» Blog - page 3"> 3 </a>
  
  
    <a href="/blog/../post/2/"> Older </a>
  
</ul>



</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
