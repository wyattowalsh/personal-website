<!DOCTYPE html>
<html lang="en-us">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>üë®‚Äçüíª Blog | Wyatt Walsh</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="üë®‚Äçüíª Blog" />
<meta name="author" content="Wyatt Walsh" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media." />
<meta property="og:description" content="Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media." />
<link rel="canonical" href="https://wwalsh.io/blog/" />
<meta property="og:url" content="https://wwalsh.io/blog/" />
<meta property="og:site_name" content="Wyatt Walsh" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="üë®‚Äçüíª Blog" />
<meta name="twitter:site" content="@wyattowalsh" />
<meta name="twitter:creator" content="@wyattowalsh" />
<script type="application/ld+json">
{"url":"https://wwalsh.io/blog/","author":{"@type":"Person","name":"Wyatt Walsh"},"description":"Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media.","@type":"WebPage","headline":"üë®‚Äçüíª Blog","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://wwalsh.io/public/favicon/favicon-32x32.png"},"name":"Wyatt Walsh"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FJCYWXT2WF"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FJCYWXT2WF');
  </script>


  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      üë®‚Äçüíª Blog &middot; Wyatt Walsh
    
  </title>

  

  <link rel="stylesheet" href="https://wwalsh.io/public/css/poole.css">
  <link rel="stylesheet" href="https://wwalsh.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://wwalsh.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> -->
  <script src="https://kit.fontawesome.com/ebaeac9722.js" crossorigin="anonymous"></script>
  <link rel="apple-touch-icon-precomposed.png" sizes="180x180" href="public/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="public/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="public/favicon/favicon-16x16.png">
  <!-- <link rel="manifest" href="/site.webmanifest"> -->
  <link rel="shortcut icon" href="public/favicon/favicon.ico">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://wwalsh.io/atom.xml">


<!-- Icons -->
<!--   <link rel="apple-touch-icon-precomposed" sizes="144x144" href="public/apple-touch-icon-precomposed.png"> -->
  <!-- <link rel="shortcut icon" href="/public/favicon.ico"> -->



  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'G-FJCYWXT2WF', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>


  <body class="theme-base-0d">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->

<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">
<!-- Toggleable sidebar -->
<body class="sidebar-overlay">
<div class="sidebar" id="sidebar">

 <!--  <br><br> 

<div id="contact-list" style="text-align:center">
    
      <a target="_blank" rel="noopener noreferrer" href="https://github.com/wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-github-alt fa-stack-1x"></i>
        </span>
      </a>
    

    
      <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-twitter fa-stack-1x"></i>
        </span>
      </a>
    

    
      <a target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-linkedin fa-stack-1x"></i>
        </span>
      </a>
    
    
      <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh@gmail.com">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-envelope fa-stack-1x"></i>
        </span>
      </a>
    
    
      <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-stack-overflow fa-stack-1x"></i>
        </span>
  
  
      <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-kaggle fa-stack-1x"></i>
        </span>
  
  
      <a target="_blank" rel="noopener noreferrer" href="mailto:@wyattowalsh">
        <span class="fa-stack fa-lg">
          <i class="fa fa-square-o fa-stack-2x"></i>
          <i class="fa fa-medium fa-stack-1x"></i>
        </span>
  
</div>
 <img align="middle" src="/public/favicon/android-chrome-512x512.png" style="text-align: center; padding-top:0;"/>
  
 -->
<div class="sidebar-item" id="contact-list" style="text-align:center; font-size: 275%; padding-bottom: 0">
  <!-- <p> Wyatt Walsh </p> -->
  <p align='center' style="margin:0; ">Wyatt Walsh</p>
</div>
<!-- <hr style="width:85%; line-height:30px; margin: auto;"> -->
<br>

<!-- 
  <div class="sidebar-item">
    <p>Personal website of Wyatt Walsh. Here, you can find more information about me, see my blog writings, view my data science notes, or see examples of my favorite media.</p>
  </div> -->
    <!-- 
  <div class="sidebar-item">
    
  </div>
 -->
  <nav class="sidebar-nav"> 
    <p style="font-size:110%; text-align:center; padding-bottom:0; margin-bottom:0;"><u><b> Sections </b></u></p>
    <hr style="width:100%; margin: auto; border: 0.25px solid;">
    <a class="sidebar-nav-item" href="https://wwalsh.io/">&#127968; Home</a>

    

    
    
      
        
        
      
    
      
        
        
      
    
      
        
        
      
    
      
        
          <a class="sidebar-nav-item" href="https://wwalsh.io/about/">ü§î About</a>
        
        
      
    
      
    
      
        
          <a class="sidebar-nav-item active" href="https://wwalsh.io/blog/">üë®‚Äçüíª Blog</a>
        
        
      
    
      
        
        
          <a target="_blank" rel="noopener noreferrer" class="sidebar-nav-item" href="https://makeuseofdata.com"> üìî Notes on Data Science</a>
        
      
    
      
    
      
    
      
        
        
      
    
      
    
    
    <!-- <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> -->
  </nav>


  <div style="text-align:center; font-size: 225%;">
    <hr style="width:100%; margin: auto; border: 0.25px solid;">
  <!--   <hr style="width:85%; margin: auto; padding-top:0;">
 -->    
<div id="social-media">
    
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="mailto:wyattowalsh@gmail.com" title="Email"><i class="fa fa-envelope"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://www.twitter.com/wyattowalsh" title="Twitter"><i class="fa fa-twitter"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://linkedin.com/in/wyattowalsh" title="LinkedIn"><i class="fa fa-linkedin"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://github.com/wyattowalsh" title="GitHub"><i class="fa fa-github"></i></a>
        
        
        <br>
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/story/wyattowalsh" title="Stackoverflow"><i class="fa fa-stack-overflow"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://kaggle.com/wyattowalsh" title="Kaggle"><i class="fab fa-kaggle"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://medium.com/@wyattowalsh" title="Medium"><i class="fa fa-medium"></i></a>
        
        
    
        
        
            <a target="_blank" rel="noopener noreferrer" href="https://open.spotify.com/user/122096382" title="Spotify"><i class="fa fa-spotify"></i></a>
        
        
    
</div>

    <p style="font-size: 42%; padding-bottom:3pt; margin-bottom:3pt;"> ~ Click Above to Connect With Me ~ </p>
    <hr style="width:100%; margin: auto; border: 0.25px solid;">
  </div>


  <div class="sidebar-item">
    <p style="position: fixed; bottom: 0;">
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>
</body>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Wyatt Walsh</a>
            <small>Reach out, let's build some cool stuff!</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="page" style="width: 100%; overflow-x: auto;">
<!--   <h1 class="page-title">üë®‚Äçüíª Blog</h1> -->
  <!-- <h2> Latest Blog Posts </h2> -->

<!-- <source url="https://medium.com/feed/@wyattowalsh">Latest Blog Posts</source> -->
<div style='background-color: #5f8fa2;'>
	<h2 style='margin-left: 12.5px; color:white;'> 
		<u>Latest Blog Posts: </u> <!-- via <a style="color:white" href='https://medium.com/@wyattowalsh"'>Medium</a> -->
	</h2>




<div style="background-color:#e1ebf0;">
	<div class="row" style="margin-left: 25px;margin-right: 25px;">
		<h3>Regularized Linear Regression Models</h3>
		<p><h4>Part Two of Three: Drawbacks of <strong><em>Ordinary Least Squares</em></strong> (OLS), Possible Remedies, and <strong>Ridge Regression</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cju3fHmuTtt5w4AFV7lfFg.png" /><figcaption>Model Coefficient Value Changes With Growing Regularization Penalty Values (<em>Image by¬†author)</em></figcaption></figure><p>Hello again and hopefully welcome back¬†üëã</p><p>In the last part of this three-part deep-dive exploration into<strong> regularized linear regression </strong>modeling techniques, several topics were covered: the equation between the response and feature variables underlying linear regression models, the sum of squared error (SSE) loss function, the O<strong><em>rdinary Least Squares </em></strong>(OLS) model, and the necessary optimization steps to find an OLS model estimator that can be trained on sample data to produce predictions of a response given new feature¬†data.</p><p>Moving forward, in this part, drawbacks of OLS, potential remedies, and the<strong> <em>Ridge Regression</em> </strong>model will be discussed.</p><p>Similar to the last part, all implementations suggested here were validated on a wine quality prediction dataset that, along with other related files, can be found at the project‚Äôs repository, <a href="http://github.com/wyattowalsh/regularized-linear-regression-deep-dive"><strong>here</strong></a><strong>.</strong></p><h4>Drawbacks of¬†OLS</h4><p>As is with most things, there are tradeoffs that have to made when modeling. One such major tradeoff is that of the <strong>bias-variance tradeoff. </strong>Any model‚Äôs error can be broken down into two components: bias and variance. Bias can be considered the error implicit in the modeling algorithm, whereas variance can be considered the error derived from differences in idiosyncrasies across training datasets. A good model is one that should have the overall error minimized, thus both bias and variance should be minimized. However, there is a tradeoff to consider as increasing bias will often decrease variance.</p><p>For the OLS model, a high variance is a concern. Since the SSE is being optimized, the model tends to fit outlier data points since they will produce higher error values due to the squared term within the loss function. By fitting these outlier points, the OLS model can subsequently base predictions off modeled patterns that are only present in the training data‚Ää‚Äî‚Ääidiosyncratic outlying points‚Ää‚Äî‚Ääand not representative of the entire population. This phenomenon is called <strong><em>overfitting</em></strong> and can lead to predictive models with low accuracy when generalizing to new predictions.</p><p>Since OLS is a low bias model, it is well-suited to have its variance lowered through bias addition, which may result in higher overall predictive ability. One way to add bias is through <strong>shrinkage</strong>, biasing model coefficient estimates toward zero. This shrinkage can be achieved through the addition of a <strong>regularization penalty </strong>to the loss function which applies a unique form of shrinkage to the overall coefficient estimates.</p><p>In the next section, I‚Äôll cover <strong><em>Ridge Regression‚Ää</em>‚Äî‚Ää</strong>regularization by the addition of a tuning parameter (Œª) coefficient controlled <strong><em>L‚ÇÇ</em></strong> penalty to the OLS loss function.</p><blockquote>Make sure to check out the next and final part of the series<strong> </strong>to learn about the other two forms of regularized linear regression, <strong>the Lasso</strong> and <strong>the Elastic¬†Net.</strong></blockquote><h3>Ridge Regression</h3><p>This form of regression is also known as <strong><em>Tikhonov Regularization </em></strong>and modifies the OLS loss function (Part One: Eq. #7) with the addition of an <strong><em>L‚ÇÇ</em></strong> penalty with an associated tuning parameter, <strong><em>Œª</em></strong>. This loss function can be described using vector notation¬†as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/616/1*BYgPXXS1fHgFUVKZQej6MA.png" /><figcaption>Equation #1</figcaption></figure><p>Similarly to the OLS case, this loss function can then be formulated as a <em>least-squares </em>optimization problem to find estimates for the model coefficients that minimize the loss function¬†as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/532/1*-MS_E_A7aT_U-3wPqQK5Pw.png" /><figcaption>Equation #2</figcaption></figure><p>Just like the OLS case, a 1/(2n) term is added in order to simply solving the gradient and allow the objective function to converge to the expected value of the model error by the Law of Large¬†Numbers.</p><p>This problem is also unconstrained and a closed-form solution for the Ridge estimator can be found by setting the gradient of the loss function (objective) equal to zero and solving the resultant equation. This produces an estimator result¬†of:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/252/1*2wlXh_tx5RcZyYvVX5DSNA.png" /><figcaption>Equation #3</figcaption></figure><p>This estimator should also be shown to be unique. In this case, the associated Hessian matrix¬†is:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/192/1*Ll-vX-C8GD2um5ZCkdknYg.png" /><figcaption>Equation #4</figcaption></figure><p>It turns out that this matrix can be shown to be positive definite¬†by:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/732/1*GdZ5Gy7DEtfBTcJQyAKakg.png" /><figcaption>Equation #5</figcaption></figure><p>Thus, since the associated Hessian matrix of the Ridge loss function is positive definite, the function is strongly convex, which implies that the Ridge estimator (Eq. #3) is the unique global minimizer to the Ridge regression problem.</p><p>Implementations of the estimator can be simplified by noticing that the problem can be reformulated as an OLS problem through data augmentation. This would take the form¬†of:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/444/1*V0A1Ci1XCkDxcqKBa8pjiA.png" /><figcaption>Equation #6</figcaption></figure><p>Thus, by utilizing the above data augmentation the same result as Equation #9 from the last part of this series can be used to solve for the coefficient estimates. That result is reproduced here:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/224/1*9J0ge0zzhKK6PrbF02x5nQ.png" /><figcaption>Equation #7</figcaption></figure><h4>Implementing the Estimator Using Python and¬†NumPy</h4><p>Similar to the OLS case, the matrix inverse does not scale well, thus the NumPy function `solve`, which employs the LAPACK <em>_gesv</em> routine, is used to find the least-squares solution. This function solves the equation in the case where A is square and full-rank (linearly independent columns). However, in the case that A is not full-rank, then the function `lstsq` should be used, which utilizes the xGELSD routine and thus finds the singular value decomposition of¬†A.</p><p>One possible Python implementation of <strong><em>Ridge Regression</em></strong> with an optional intercept term¬†is:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0b41872848020b8c372ce47012744a02/href">https://medium.com/media/0b41872848020b8c372ce47012744a02/href</a></iframe><h3>Conclusion</h3><p>Thanks for reading part two of <strong><em>Regularized Linear Regression Models</em></strong>!¬†üôå</p><p>If you have not already, make sure to check out <a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c"><strong>part¬†one</strong>!</a></p><p>Continue on to <strong>part three </strong>to learn about <strong>the Lasso </strong>and <strong>the Elastic Net</strong>, the last two regularized linear regression techniques!</p><p>See <a href="https://github.com/wyattowalsh/regularized-linear-regression-deep-dive/blob/master/SOURCES.md"><strong>here</strong></a> for the different sources utilized to create this series of¬†posts.</p><p>Please leave a comment if you would like! I am always trying to improve my posts (logically, syntactically, or otherwise) and am happy to discuss anything related!¬†üëç</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=44572e79a1b5" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/regularized-linear-regression-models-44572e79a1b5">Regularized Linear Regression Models</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
		<br>
	

<div style="background-color:#e1ebf0;">
	<div class="row" style="margin-left: 25px;margin-right: 25px;">
		<h3>Regularized Linear Regression Models</h3>
		<p><h4>Part One of Three: Introduction, Linear Regression Modeling, and Ordinary Least Squares¬†(OLS)</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cju3fHmuTtt5w4AFV7lfFg.png" /><figcaption>Model Coefficient Value Changes With Growing Regularization Penalty Values (<em>Image by¬†author)</em></figcaption></figure><p>Hey üëã</p><p>Welcome to part one of a three-part deep-dive on <strong>regularized linear regression modeling‚Ää‚Äî‚Ää</strong>some of the most popular algorithms for supervised learning¬†tasks<strong>.</strong></p><p>Before hopping into the equations and code, let us first discuss what will be covered in this¬†series.</p><p><strong>Part one </strong>will include an introductory discussion about regression, an explanation of linear regression modeling, and a presentation of the <strong><em>Ordinary Least Squares</em> </strong>(OLS) model (from the derivation of the model estimator using applied optimization theory through the implementation of the findings in Python using¬†NumPy).</p><p>Drawbacks of the OLS model and some possible remedies will be discussed in <strong>part two</strong>. One such remedy, <strong>Ridge regression</strong>, will be presented here with an explanation including the derivation of its model estimator and NumPy implementation in¬†Python.</p><p><strong>Part three </strong>will conclude this series of posts with explanations of<strong> </strong>the remaining regularized linear models: <strong>the Lasso </strong>and <strong>the Elastic Net</strong>. Solving these models is more complicated than in previous cases since a discrete optimization technique is needed. The cause of this complication, the <em>Pathwise Coordinate Descent </em>algorithm along with its NumPy-based Python implementation, and some concluding remarks are given in this¬†post.</p><p>The models and included implementations were tested on a wine quality prediction dataset of which the code and results can be viewed at the project repository <a href="http://github.com/wyattowalsh/regularized-linear-regression-deep-dive"><strong>here</strong></a></p><h3>Introduction</h3><p>Managerial decision making, organizational efficiency, and revenue generation are all areas that can be improved through the utilization of data-based insights. Currently, these insights are being more readily sought out as technological accessibility stretches further and competitive advantages in the market are harder to acquire. One field that seeks to realize value within collected data samples is predictive analytics. By leveraging mathematical/statistical techniques and programming, practitioners are able to identify patterns within data allowing for the generation of valuable insights.</p><p><strong><em>Regression</em></strong> is one technique within predictive analytics that is used to predict the value of a continuous response variable given one or many related feature variables. Algorithms of this class accomplish this task by learning the relationships between the input (feature) variables and the output (response) variable through training on a sample dataset. How these relationships are learned, and furthermore used for prediction varies from algorithm to algorithm. The practitioner is faced with options for regression modeling algorithms, however, linear regression models tend to be explored early on in the process due to their ease of application and high explainability.</p><h3>Linear Regression Modeling</h3><p>A linear regression model learns the input-output relationships by fitting a linear function to the sample data. This can be mathematically formalized as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-mNqFuCYxsd5V-4iXhhf1A.png" /><figcaption>Equation #1</figcaption></figure><p>Thus, the response is modeled as a weighted sum of the input variables multiplied by linear coefficients with an error term included. It will prove useful in future steps involving optimization to use vector notation. The linear modeling equation can be expressed this way¬†as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uyVuwzY3CjR6-02BzYYkBw.png" /><figcaption>Equation #2</figcaption></figure><p>An important aspect of the above equation to note is that there is a column of <strong>1</strong>‚Äôs appended to the design matrix. This is such that the first coefficient of the coefficient vector can serve as an intercept term. In cases where an intercept is not sought after this column can be¬†omitted.</p><p>Thus the goal of model training is to find an estimate of the coefficient vector, <strong><em>Œ≤ÃÇ, </em></strong>which can then be utilized with the above equations to make predictions of the response given new feature data. This can be accomplished by applying optimization theory to the model equations above to derive an equation for the model coefficient estimator that minimizes a notion of model error found by training on the sample¬†data.</p><h4>Minimizing a Notion of Model¬†Error</h4><p>To consider how model error can be minimized, a consideration of model error must first be made. Prediction error for a single prediction can be expressed as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9vEjdXmGaIZEcNd05V6LJw.png" /><figcaption>Equation #3</figcaption></figure><p>Thus, in vector notation, total model error across all predictions can be found¬†as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/884/1*uCfmafhvNzFh-DVtabK5Ow.png" /><figcaption>Equation #4</figcaption></figure><p>However, for the uses of finding a minimal overall model error, the <strong><em>L‚ÇÇ</em></strong> norm above is not a good objective function. This is due to the fact that negative errors and positive errors will cancel out, thus a minimization will find an objective value of zero even though in reality the model error is much¬†higher.</p><p>This signed error cancellation issue can be solved by squaring the model‚Äôs prediction error producing the sum of squared error (SSE)¬†term:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/376/1*F6kywmhTdtdO9jdlDMV4nw.png" /><figcaption>Equation #5</figcaption></figure><p>This same term can be expressed in vector notation¬†as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/240/1*aI5pkAoiEMATRjC3WgNx7Q.png" /><figcaption>Equation #6</figcaption></figure><p>As will be seen in future optimization applications, this function is much better suited to serve as a <strong>loss function</strong>, a function minimized that aptly models the error for a given technique.<strong> </strong>Many different models other than regularized linear models use the SSE error term as a term in their respective loss functions.</p><h3><strong>Ordinary Least¬†Squares</strong></h3><p>Now that linear modeling and error has been covered, we can move on to the most simple linear regression model, <strong><em>Ordinary Least Squares</em></strong> (OLS). In this case, the simple SSE error term is the model‚Äôs loss function and can be expressed as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/324/1*uRoHZOW3Y5tMltkfSldT7w.png" /><figcaption>Equation #7</figcaption></figure><p>Using this loss function, the problem can now be formalized as a <em>least-squares </em>optimization problem. This problem serves to derive estimates for the model parameters, <strong><em>Œ≤</em></strong>, that minimize the SSE between the actual and predicted values of the outcome and is formalized as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/440/1*_TrQb8nOOANLCti93ydumg.png" /><figcaption>Equation #8</figcaption></figure><p>The 1/(2n) term is added in order to simply solving the gradient and allow the objective function to converge to the expected value of the model error by the Law of Large¬†Numbers.</p><p>Aided by the problem‚Äôs unconstrained nature, a closed-form solution for the OLS estimator can be obtained by setting the gradient of the loss function (objective) equal to zero and solving the resultant equation for the coefficient vector, <strong><em>Œ≤ÃÇ. </em></strong>This produces the following estimator:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/224/1*9J0ge0zzhKK6PrbF02x5nQ.png" /><figcaption>Equation #9</figcaption></figure><p>However, this may not be the only optimal estimator, thus its uniqueness should be proven. To do this, it will suffice to show that the loss function (Eq. #8) is convex since any local optimality of a convex function is also global optimality and therefore unique.</p><p>One possible way to show this is through the second-order convexity conditions, which state that a function is convex if it is continuous, twice differentiable, and has an associated Hessian matrix that is positive semi-definite. Due to its quadratic nature, the OLS loss function (Eq. #8) is both continuous and twice differentiable, satisfying the first two conditions.</p><p>To establish the last condition, the OLS Hessian matrix is found¬†as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/124/1*mmRm7_x5xCuUFkfbqxD7wg.png" /><figcaption>Equation #10</figcaption></figure><p>Furthermore, this Hessian can be shown to be positive semi-definite as:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/504/1*xmTZLUJYNwqz2mxGmwhYLQ.png" /><figcaption>Equation #11</figcaption></figure><p>Thus, by the second-order conditions for convexity, the OLS loss function (Eq. #8) is convex, thus the estimator found above (Eq #9) is the unique global minimizer to the OLS¬†problem.</p><h4>Implementing the Estimator Using Python and¬†NumPy</h4><p>Solving for the OLS estimator using the matrix inverse does not scale well, thus the NumPy function `solve`, which employs the LAPACK <em>_gesv</em> routine, is used to find the least-squares solution. This function solves the equation in the case where A is square and full-rank (linearly independent columns). However, in the case that A is not full-rank, then the function `lstsq` should be used, which utilizes the xGELSD routine and thus finds the singular value decomposition of¬†A.</p><p>One possible implementation in Python of OLS with an optional intercept term¬†is:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/765cf2f243fe80cc6fb7b374bdf8dd5f/href">https://medium.com/media/765cf2f243fe80cc6fb7b374bdf8dd5f/href</a></iframe><h3><strong>Conclusion</strong></h3><p>Hope you enjoyed part one of <strong><em>Regularized Linear Regression Models.¬†</em>üëç</strong></p><p>Make sure to check out <strong>part two</strong> to find out why the OLS model sometimes fails to perform accurately and how <strong><em>Ridge Regression</em></strong> can be used to¬†help!</p><p>See <a href="https://github.com/wyattowalsh/regularized-linear-regression-deep-dive/blob/master/SOURCES.md"><strong>here</strong></a> for the different sources utilized to create this series of¬†posts.</p><p>Please leave a comment if you would like! I am always trying to improve my posts (logically, syntactically, or otherwise) and am happy to discuss anything related!¬†üòä</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=57bbdce90a8c" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/regularized-linear-regression-models-57bbdce90a8c">Regularized Linear Regression Models</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
		<br>
	
	</div>
</div>
</div>
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
